{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e1a4ff05",
      "metadata": {
        "id": "e1a4ff05"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -U -q\n",
        "import torch\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "462b103f",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "462b103f",
        "outputId": "e3839d51-b13d-442e-8abc-9c4951f9701d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "! pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "49b92691",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49b92691",
        "outputId": "90778d12-a587-48d5-841f-02a7a208f266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers==4.35.0\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "4a80f4de",
      "metadata": {
        "id": "4a80f4de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68096be7-4dc0-4c14-a99f-9d0d5e5d1d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comment खण्ड में अपनी प्रतिक्रिया को निश्‍चित ही share करें ।\n",
            "तो यह एक बड़ा video है, तो भी मैं स्पष्ट रूप से सभी उत्पादों का mention करेंगे।\n",
            "मैं अपने बैग के लिए इंतजार कर रहा था.\n"
          ]
        }
      ],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-hi'\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def is_noun(text):\n",
        "    ans = nltk.pos_tag([text])\n",
        "    val = ans[0][1]\n",
        "\n",
        "    if val in ('NN', 'NNS', 'NNPS', 'NNP'):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "input_text = [\"Definitely share your feedback in the comment section.\",\n",
        "              \"So even if it's a big video, I will clearly mention all the products.\",\n",
        "              \"I was waiting for my bag.\"]\n",
        "\n",
        "\n",
        "def translate_and_preserve_nouns(input_text):\n",
        "    translated_sentences = []\n",
        "\n",
        "    for text in input_text:\n",
        "        model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        generated_tokens = model.generate(**model_inputs)\n",
        "        translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "        words = text.split()\n",
        "        noun_list = [word for word in words if is_noun(word)]\n",
        "\n",
        "        for noun in noun_list:\n",
        "            noun = noun.replace(',', '')\n",
        "            model_inputs_noun = tokenizer(noun, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            generated_tokens_noun = model.generate(**model_inputs_noun)\n",
        "            noun_translation = tokenizer.decode(generated_tokens_noun[0], skip_special_tokens=True)\n",
        "            translation = translation.replace(noun_translation, noun)\n",
        "\n",
        "        translated_sentences.append(translation)\n",
        "\n",
        "    return translated_sentences\n",
        "\n",
        "translated_sentences = translate_and_preserve_nouns(input_text)\n",
        "for sentence in translated_sentences:\n",
        "    print(sentence)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_qualitatively(translated_sentences, input_text):\n",
        "    print(\"Original English Text:\")\n",
        "    for text in input_text:\n",
        "        print(text)\n",
        "\n",
        "    print(\"\\nTranslated Hinglish Text:\")\n",
        "    for sentence in translated_sentences:\n",
        "        print(sentence)\n",
        "\n",
        "evaluate_qualitatively(translated_sentences, input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsa0nc8mcgJ-",
        "outputId": "023f727f-d455-4a20-cffd-92dfaa2a1eb0"
      },
      "id": "tsa0nc8mcgJ-",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original English Text:\n",
            "Definitely share your feedback in the comment section.\n",
            "So even if it's a big video, I will clearly mention all the products.\n",
            "I was waiting for my bag.\n",
            "\n",
            "Translated Hinglish Text:\n",
            "comment खण्ड में अपनी प्रतिक्रिया को निश्‍चित ही share करें ।\n",
            "तो यह एक बड़ा video है, तो भी मैं स्पष्ट रूप से सभी उत्पादों का mention करेंगे।\n",
            "मैं अपने बैग के लिए इंतजार कर रहा था.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yMNc1ut7lMlh"
      },
      "id": "yMNc1ut7lMlh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}